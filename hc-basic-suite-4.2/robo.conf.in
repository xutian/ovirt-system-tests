# A sample configuration file to setup ROBO
[hosts]
@HOST0@
@HOST1@
@HOST2@

[disktype]
raid6

[diskcount]
4

[stripesize]
256

# Installing via reposync and setup hosts instead to reduce time to download packages via gdeploy
#[yum1]
#action=install
#gpgcheck=no
#repos=https://download.gluster.org/pub/gluster/glusterfs-nagios/1.1.0/CentOS/epel-7Server/,http://mirror.centos.org/centos/7/updates/x86_64,http://mirror.centos.org/centos/7/os/x86_64,http://download.fedoraproject.org/pub/epel/7/x86_64

[pv]
action=create
devices=sda

[vg1]
action=create
vgname=RHS_vg1
pvname=sda

[lv1]
action=create
vgname=RHS_vg1
lvname=engine_lv
lvtype=thick
size=100GB
mount=/rhs/brick1

[lv2]
action=create
vgname=RHS_vg1
poolname=lvthinpool
lvtype=thinpool
poolmetadatasize=10MB
chunksize=1024k
size=45GB

[lv3]
action=create
lvname=lv_vmaddldisks
poolname=lvthinpool
vgname=RHS_vg1
lvtype=thinlv
mount=/rhs/brick2
virtualsize=30GB

[lv4]
action=create
lvname=lv_vmrootdisks
poolname=lvthinpool
vgname=RHS_vg1
size=19GB
lvtype=thinlv
mount=/rhs/brick3
virtualsize=20GB

#Commenting out now, because of issues with guest-fs-freeze
#[selinux]
#yes

[vg2]
action=extend
vgname=RHS_vg1
pvname=sdb

[lv5]
action=setup-cache
ssd=sdb
vgname=RHS_vg1
poolname=lvthinpool
cache_meta_lv=lvcachemeta
cache_lv=lvcache
cache_meta_lvsize=1GB
cache_lvsize=5GB

# shell3, shell4, shell5 setups the glusterfs.slice with CPUQuota 400%
[shell3]
action=execute
command=mkdir /etc/systemd/system/glusterd.service.d,

[shell4]
action=execute
command=echo -e "[Service]\nCPUAccounting=yes\nSlice=glusterfs.slice" >> /etc/systemd/system/glusterd.service.d/99-cpu.conf,

[shell5]
action=execute
command=echo -e "[Slice]\nCPUQuota=400%" >> /etc/systemd/system/glusterfs.slice,systemctl daemon-reload

[service1]
action=restart
service=glusterd

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp
services=glusterfs

[script]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-gluster-hooks.sh

[volume1]
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick1/engine

[volume2]
action=create
volname=vmstore
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick2/vmstore

[volume3]
action=create
volname=data
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,cluster.locking-scheme,cluster.shd-max-threads,cluster.shd-wait-qlength,performance.strict-o-direct,network.remote-dio,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,on,off,30,off,on
brick_dirs=/rhs/brick3/data

#[yum2:@HOST0@]
#action=install
#gpgcheck=no
#packages=ovirt-engine-appliance

[script:@HOST0@]
action=execute
file=/root/generate-hc-answerfile.sh @HOST0@ @HOST1@ @HOST2@ @HOSTEDENGINE@ @DOMAIN@

[shell2:@HOST0@]
action=execute
command=hosted-engine --deploy --config-append=/root/hc-answers.conf
